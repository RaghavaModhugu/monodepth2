{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "from kitti_utils import generate_depth_map\n",
    "\n",
    "import argoverse\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir =  '/Users/raghavamodhugu/Downloads/Argoverse_samples/argoverse-tracking/sample'\n",
    "# argoverse_loader = ArgoverseTrackingLoader(root_dir)\n",
    "# print('Total number of logs:',len(argoverse_loader))\n",
    "# argoverse_loader.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argoverse_load_image(argoverse_data, camera, folder, frame_index):\n",
    "    img = argoverse_data.get_image_sync(idx,camera = camera)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argo_MonoDataset(data.Dataset):\n",
    "    \"\"\"Superclass for monocular dataloaders\n",
    "\n",
    "    Args:\n",
    "        data_path\n",
    "        filenames\n",
    "        height\n",
    "        width\n",
    "        frame_idxs\n",
    "        num_scales\n",
    "        is_train\n",
    "        img_ext\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 filenames,\n",
    "                 height,\n",
    "                 width,\n",
    "                 frame_idxs,\n",
    "                 num_scales,\n",
    "                 is_train=False,\n",
    "                 img_ext='.jpg'):\n",
    "        super(MonoDataset, self).__init__()\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.filenames = filenames\n",
    "        self.height = height #2056\n",
    "        self.width = width #2464\n",
    "        self.num_scales = num_scales\n",
    "        self.interp = Image.ANTIALIAS\n",
    "\n",
    "        self.frame_idxs = frame_idxs\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.img_ext = img_ext\n",
    "\n",
    "        self.loader = argoverse_load_image\n",
    "        self.camera = argoverse_loader.CAMERA_LIST[-2]\n",
    "        self.argoverse_loader = ArgoverseTrackingLoader(self.data_path)\n",
    "        self.argoverse_data = 0\n",
    "        \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "        # We need to specify augmentations differently in newer versions of torchvision.\n",
    "        # We first try the newer tuple version; if this fails we fall back to scalars\n",
    "        try:\n",
    "            self.brightness = (0.8, 1.2)\n",
    "            self.contrast = (0.8, 1.2)\n",
    "            self.saturation = (0.8, 1.2)\n",
    "            self.hue = (-0.1, 0.1)\n",
    "            transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        except TypeError:\n",
    "            self.brightness = 0.2\n",
    "            self.contrast = 0.2\n",
    "            self.saturation = 0.2\n",
    "            self.hue = 0.1\n",
    "\n",
    "        self.resize = {}\n",
    "        for i in range(self.num_scales):\n",
    "            s = 2 ** i\n",
    "            self.resize[i] = transforms.Resize((self.height // s, self.width // s),\n",
    "                                               interpolation=self.interp)\n",
    "\n",
    "        self.load_depth = self.check_depth()\n",
    "\n",
    "    def preprocess(self, inputs, color_aug):\n",
    "        \"\"\"Resize colour images to the required scales and augment if required\n",
    "\n",
    "        We create the color_aug object in advance and apply the same augmentation to all\n",
    "        images in this item. This ensures that all images input to the pose network receive the\n",
    "        same augmentation.\n",
    "        \"\"\"\n",
    "        for k in list(inputs):\n",
    "            frame = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                for i in range(self.num_scales):\n",
    "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
    "\n",
    "        for k in list(inputs):\n",
    "            f = inputs[k]\n",
    "            if \"color\" in k:\n",
    "                n, im, i = k\n",
    "                inputs[(n, im, i)] = self.to_tensor(f)\n",
    "                inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns a single training item from the dataset as a dictionary.\n",
    "\n",
    "        Values correspond to torch tensors.\n",
    "        Keys in the dictionary are either strings or tuples:\n",
    "\n",
    "            (\"color\", <frame_id>, <scale>)          for raw colour images,\n",
    "            (\"color_aug\", <frame_id>, <scale>)      for augmented colour images,\n",
    "            (\"K\", scale) or (\"inv_K\", scale)        for camera intrinsics,\n",
    "            \"stereo_T\"                              for camera extrinsics, and\n",
    "            \"depth_gt\"                              for ground truth depth maps.\n",
    "\n",
    "        <frame_id> is either:\n",
    "            an integer (e.g. 0, -1, or 1) representing the temporal step relative to 'index',\n",
    "        or\n",
    "            \"s\" for the opposite image in the stereo pair.\n",
    "\n",
    "        <scale> is an integer representing the scale of the image relative to the fullsize image:\n",
    "            -1      images at native resolution as loaded from disk\n",
    "            0       images resized to (self.width,      self.height     )\n",
    "            1       images resized to (self.width // 2, self.height // 2)\n",
    "            2       images resized to (self.width // 4, self.height // 4)\n",
    "            3       images resized to (self.width // 8, self.height // 8)\n",
    "        \"\"\"\n",
    "        inputs = {}\n",
    "\n",
    "        do_color_aug = self.is_train and random.random() > 0.5\n",
    "        do_flip = self.is_train and random.random() > 0.5\n",
    "\n",
    "        line = self.filenames[index].split()\n",
    "        folder = line[0]\n",
    "        self.argoverse_data = argoverse_loader[int(folder)]\n",
    "        last_row = np.array([[0.0, 0.0, 0.0, 1.0]])\n",
    "        K_ = self.argoverse_data.get_calibration(camera).K\n",
    "        K = np.vstack((K_, last_row))\n",
    "        H = 2056\n",
    "        W = 2464\n",
    "        K[0] = K[0]/W\n",
    "        K[1] = K[1]/H\n",
    "\n",
    "        if len(line) == 3:\n",
    "            frame_index = int(line[1])\n",
    "        else:\n",
    "            frame_index = 0\n",
    "\n",
    "        if len(line) == 3:\n",
    "            side = line[2]\n",
    "        else:\n",
    "            side = None\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            if i == \"s\":\n",
    "                other_side = {\"r\": \"l\", \"l\": \"r\"}[side]\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(self.argoverse_data, folder, frame_index, other_side, do_flip)\n",
    "            else:\n",
    "                inputs[(\"color\", i, -1)] = self.get_color(self.argoverse_data, folder, frame_index + i, side, do_flip)\n",
    "\n",
    "        # adjusting intrinsics to match each scale in the pyramid\n",
    "        for scale in range(self.num_scales):\n",
    "\n",
    "            K[0, :] *= self.width // (2 ** scale)\n",
    "            K[1, :] *= self.height // (2 ** scale)\n",
    "\n",
    "            inv_K = np.linalg.pinv(K)\n",
    "\n",
    "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
    "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
    "\n",
    "        if do_color_aug:\n",
    "            color_aug = transforms.ColorJitter.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "        else:\n",
    "            color_aug = (lambda x: x)\n",
    "\n",
    "        self.preprocess(inputs, color_aug)\n",
    "\n",
    "        for i in self.frame_idxs:\n",
    "            del inputs[(\"color\", i, -1)]\n",
    "            del inputs[(\"color_aug\", i, -1)]\n",
    "\n",
    "        if self.load_depth:\n",
    "            depth_gt = self.get_depth(folder, frame_index, side, do_flip)\n",
    "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
    "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
    "\n",
    "        if \"s\" in self.frame_idxs:\n",
    "            stereo_T = np.eye(4, dtype=np.float32)\n",
    "            baseline_sign = -1 if do_flip else 1\n",
    "            side_sign = -1 if side == \"l\" else 1\n",
    "            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n",
    "\n",
    "            inputs[\"stereo_T\"] = torch.from_numpy(stereo_T)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def check_depth(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_depth(self, folder, frame_index, side, do_flip):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Argo_MonoDataset):\n",
    "    \"\"\"Superclass for different types of KITTI dataset loaders\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(KITTIDataset, self).__init__(*args, **kwargs)\n",
    "        self.full_res_shape = (1242, 375)\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "\n",
    "    def check_depth(self):\n",
    "        return False\n",
    "\n",
    "    def get_color(self, folder, frame_index, side, do_flip):\n",
    "        color = self.loader(self.argoverse_data, self.camera, folder, frame_index)\n",
    "        if do_flip:\n",
    "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return color\n",
    "    \n",
    "    def get_depth(self, folder, frame_index, side, do_flip):\n",
    "        img_path = self.argoverse_data.get_image(60, camera, load=False)\n",
    "        img = self.argoverse_data.get_image(60, camera, load=True)\n",
    "        Lidar_id = self.argoverse_data.image_list_sync[camera].index(img_path)\n",
    "        calib = self.argoverse_data.get_calibration(camera)\n",
    "        pc = self.argoverse_data.get_lidar(Lidar_id)\n",
    "        uv = calib.project_ego_to_image(pc).T\n",
    "        idx_ = np.where(np.logical_and.reduce((uv[0, :] >= 0.0, uv[0, :] < np.shape(img)[1] - 1.0,\n",
    "                                                              uv[1, :] >= 0.0, uv[1, :] < np.shape(img)[0] - 1.0,\n",
    "                                                              uv[2, :] > 0)))\n",
    "        idx_ = idx_[0]\n",
    "        uv1 =uv[:, idx_]\n",
    "        if do_flip:\n",
    "            uv1 = np.fliplr(uv1)\n",
    "\n",
    "        return uv1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
